#Experiments Overview

This experiment introduces the basic building blocks of neural networks such as neurons, weights, bias, activation functions, and loss functions.  
The objective is to understand how a simple neural network learns patterns from data using gradient descent.

**Key Learnings:**
- Working of a perceptron
- Role of activation functions
- Concept of training and loss minimization

---

In this experiment, a multi-layer Artificial Neural Network (ANN) is implemented to classify **linearly separable** and **non-linearly separable** datasets.  
The experiment highlights why single-layer networks fail on non-linear data and how hidden layers help overcome this limitation.

**Key Learnings:**
- Difference between linear and non-linear separability
- Importance of hidden layers
- Performance comparison of ANN models

---

This experiment involves implementing **Convolutional Neural Networks (CNNs)** for image classification using the **Cats vs Dogs** and **CIFAR-10** datasets.  
Various CNN configurations are tested by experimenting with:
- Different activation functions (ReLU, Tanh, Leaky ReLU)
- Weight initialization techniques (Xavier, Kaiming, Random)
- Optimizers (SGD, Adam, RMSprop)

Additionally, **transfer learning** is performed using a pretrained **ResNet-18** model, and its performance is compared with a custom CNN.

**Key Learnings:**
- CNN architecture design
- Role of convolution, pooling, and dropout layers
- Impact of hyperparameter tuning
- Advantages of pretrained deep learning models

---

#Technologies and Tools Used
- **Python**
- **PyTorch**
- **NumPy**
- **Torchvision**
- **Google Colab**
- **GitHub**
