{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceType": "datasetVersion",
          "sourceId": 14866150,
          "datasetId": 9509889,
          "databundleVersionId": 15727776
        }
      ],
      "dockerImageVersionId": 31287,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishikasahni/Deep-Learning/blob/main/Deep_Learning_Assignment_7_Sequence_to_Sequence_Learning_with_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "data_path = \"/kaggle/input/datasets/krishikasahni/spatxt/spa.txt\"\n",
        "lines = open(data_path, encoding=\"utf-8\").read().split(\"\\n\")[:20000]\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "for line in lines:\n",
        "    if \"\\t\" in line:\n",
        "        eng, spa = line.split(\"\\t\")[:2]\n",
        "        input_texts.append(eng)\n",
        "        target_texts.append(\"\\t \" + spa + \" \\n\")\n",
        "\n",
        "num_words = 5000\n",
        "\n",
        "input_tok = Tokenizer(num_words=num_words, filters='')\n",
        "input_tok.fit_on_texts(input_texts)\n",
        "input_seq = input_tok.texts_to_sequences(input_texts)\n",
        "\n",
        "target_tok = Tokenizer(num_words=num_words, filters='')\n",
        "target_tok.fit_on_texts(target_texts)\n",
        "target_seq = target_tok.texts_to_sequences(target_texts)\n",
        "\n",
        "max_enc = max(len(s) for s in input_seq)\n",
        "max_dec = max(len(s) for s in target_seq)\n",
        "\n",
        "encoder_input = pad_sequences(input_seq, maxlen=max_enc, padding='post')\n",
        "decoder_input = pad_sequences(target_seq, maxlen=max_dec, padding='post')\n",
        "\n",
        "decoder_target = np.zeros_like(decoder_input)\n",
        "decoder_target[:, :-1] = decoder_input[:, 1:]\n",
        "\n",
        "latent_dim = 128\n",
        "\n",
        "enc_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(num_words, latent_dim)(enc_inputs)\n",
        "_, state_h, state_c = LSTM(latent_dim, return_state=True)(enc_emb)\n",
        "enc_states = [state_h, state_c]\n",
        "\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_words, latent_dim)\n",
        "dec_emb = dec_emb_layer(dec_inputs)\n",
        "\n",
        "dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "dec_outputs, _, _ = dec_lstm(dec_emb, initial_state=enc_states)\n",
        "dec_dense = Dense(num_words, activation='softmax')\n",
        "dec_outputs = dec_dense(dec_outputs)\n",
        "\n",
        "model = Model([enc_inputs, dec_inputs], dec_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "    [encoder_input, decoder_input],\n",
        "    np.expand_dims(decoder_target, -1),\n",
        "    batch_size=128,\n",
        "    epochs=5,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "encoder_model = Model(enc_inputs, enc_states)\n",
        "\n",
        "dec_state_input_h = Input(shape=(latent_dim,))\n",
        "dec_state_input_c = Input(shape=(latent_dim,))\n",
        "dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "\n",
        "dec_emb2 = dec_emb_layer(dec_inputs)\n",
        "dec_outputs2, state_h2, state_c2 = dec_lstm(dec_emb2, initial_state=dec_states_inputs)\n",
        "dec_states2 = [state_h2, state_c2]\n",
        "dec_outputs2 = dec_dense(dec_outputs2)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [dec_inputs] + dec_states_inputs,\n",
        "    [dec_outputs2] + dec_states2\n",
        ")\n",
        "\n",
        "def translate_sentence(sentence):\n",
        "    seq = input_tok.texts_to_sequences([sentence])\n",
        "    seq = pad_sequences(seq, maxlen=max_enc, padding='post')\n",
        "\n",
        "    states = encoder_model.predict(seq, verbose=0)\n",
        "\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0,0] = target_tok.word_index['\\t']\n",
        "\n",
        "    decoded_sentence = ''\n",
        "    while True:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states, verbose=0)\n",
        "        idx = np.argmax(output_tokens[0, -1, :])\n",
        "        word = target_tok.index_word.get(idx, '')\n",
        "\n",
        "        if word == '\\n' or len(decoded_sentence.split()) > 20:\n",
        "            break\n",
        "\n",
        "        decoded_sentence += ' ' + word\n",
        "        target_seq[0,0] = idx\n",
        "        states = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nEnter English sentence (or type 'exit'): \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "    print(\"Spanish:\", translate_sentence(user_input))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-20T09:03:58.894323Z",
          "iopub.execute_input": "2026-02-20T09:03:58.895071Z",
          "iopub.status.idle": "2026-02-20T09:04:56.541229Z",
          "shell.execute_reply.started": "2026-02-20T09:03:58.895029Z",
          "shell.execute_reply": "2026-02-20T09:04:56.540628Z"
        },
        "id": "8aCpZudodFSO",
        "outputId": "7828c02d-603f-4c00-d3f2-65a094955af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/5\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.6726 - loss: 4.5759 - val_accuracy: 0.6589 - val_loss: 2.0742\nEpoch 2/5\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.7526 - loss: 1.6588 - val_accuracy: 0.7356 - val_loss: 1.8884\nEpoch 3/5\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.7855 - loss: 1.5167 - val_accuracy: 0.7401 - val_loss: 1.8050\nEpoch 4/5\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.7891 - loss: 1.4411 - val_accuracy: 0.7415 - val_loss: 1.7497\nEpoch 5/5\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.7916 - loss: 1.3757 - val_accuracy: 0.7485 - val_loss: 1.6943\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "\nEnter English sentence (or type 'exit'):  How are you?\n"
        },
        {
          "name": "stdout",
          "text": "Spanish: yo me\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "\nEnter English sentence (or type 'exit'):  I am good\n"
        },
        {
          "name": "stdout",
          "text": "Spanish: me a\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "\nEnter English sentence (or type 'exit'):  exit\n"
        }
      ],
      "execution_count": null
    }
  ]
}